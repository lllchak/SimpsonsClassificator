{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpsons_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg0sJl9THDtM",
        "outputId": "c2968146-cc99-4109-a211-8acd84e56bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "from tqdm.autonotebook import tqdm, trange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid', font_scale=1.4)\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "rescale_size = 244\n",
        "batch_size = 32\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((int(rescale_size * 1.25), int(rescale_size * 1.25))),\n",
        "    transforms.RandomCrop(rescale_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_data = datasets.ImageFolder(\n",
        "    root='../journey-springfield/train/simpsons_dataset',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print('Data folded successfully!\\n')\n",
        "\n",
        "train_idx, valid_idx = train_test_split(list(range(len(full_data))), train_size=0.8)\n",
        "dataset = {\n",
        "    'train': torch.utils.data.Subset(full_data, train_idx),\n",
        "    'val': torch.utils.data.Subset(full_data, valid_idx)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(\n",
        "        dataset=dataset['train'], batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    ),\n",
        "    'val': torch.utils.data.DataLoader(\n",
        "        dataset=dataset['val'], batch_size=batch_size, shuffle=False, num_workers=2\n",
        "    ),\n",
        "}\n",
        "\n",
        "datasets_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
        "class_names = np.array(full_data.classes)\n",
        "\n",
        "print('There are such classes in full data:\\n')\n",
        "for i, el in enumerate(class_names):\n",
        "    print(f'{i+1}. {el}')\n",
        "\n",
        "print('\\nDatasets sizes:', datasets_sizes, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4l1AlthHPvN",
        "outputId": "4227d8da-8919-4d58-d983-bc6acba998fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  journey-springfield.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of journey-springfield.zip or\n",
            "        journey-springfield.zip.zip, and cannot find journey-springfield.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((int(rescale_size * 1.25), int(rescale_size * 1.25))),\n",
        "    transforms.RandomCrop(rescale_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_data = datasets.ImageFolder(\n",
        "    root='../journey-springfield/train/simpsons_dataset',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print('Data folded successfully!\\n')\n",
        "\n",
        "train_idx, valid_idx = train_test_split(list(range(len(full_data))), train_size=0.8)\n",
        "dataset = {\n",
        "    'train': torch.utils.data.Subset(full_data, train_idx),\n",
        "    'val': torch.utils.data.Subset(full_data, valid_idx)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(\n",
        "        dataset=dataset['train'], batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    ),\n",
        "    'val': torch.utils.data.DataLoader(\n",
        "        dataset=dataset['val'], batch_size=batch_size, shuffle=False, num_workers=2\n",
        "    ),\n",
        "}\n",
        "\n",
        "datasets_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
        "class_names = np.array(full_data.classes)\n",
        "\n",
        "print('There are such classes in full data:\\n')\n",
        "for i, el in enumerate(class_names):\n",
        "    print(f'{i+1}. {el}')\n",
        "\n",
        "print('\\nDatasets sizes:', datasets_sizes, '\\n')"
      ],
      "metadata": {
        "id": "lYPajowcIDXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inpt, title=None):\n",
        "    inpt = inpt.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.299, 0.224, 0.225])\n",
        "    inpt = std * inpt + mean\n",
        "    inpt = np.clip(inpt, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    plt.imshow(inpt)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "inputs, classes = next(iter(dataloader['train']))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out)"
      ],
      "metadata": {
        "id": "PQW-iRo0xpGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for img in dataloader['train']:\n",
        "    print(img[0][0])\n",
        "    break"
      ],
      "metadata": {
        "id": "6ZbrZ9s8xpIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, epochs=25):\n",
        "    '''\n",
        "\n",
        "    main training code\n",
        "\n",
        "    '''\n",
        "    start = time.time()\n",
        "    \n",
        "    use_gpu = torch.cuda.is_available()\n",
        "  \n",
        "    best_mode_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    losses = {'train': [], 'val': []}\n",
        "    accs = {'train': [], 'val': []}\n",
        "\n",
        "    pbar = trange(epochs, desc='Epoch')\n",
        "\n",
        "    for epoch in pbar:\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train(True)\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            curr_loss = 0.0\n",
        "            curr_corrects = 0  \n",
        "\n",
        "            for data in tqdm(dataloader[phase], leave=False, desc=f'{phase} iter'):\n",
        "                inputs, labels = data\n",
        "                if use_gpu:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                else:\n",
        "                    inputs, labels = inputs, labels\n",
        "        \n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if phase == 'val':\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(inputs)\n",
        "                else:\n",
        "                    outputs = model(inputs)\n",
        "\n",
        "                preds = torch.argmax(outputs, -1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                curr_loss += loss.item()\n",
        "                curr_corrects += int(torch.sum(preds == labels.data))\n",
        "\n",
        "            epoch_loss = curr_loss / datasets_sizes[phase]\n",
        "            epoch_acc = curr_corrects / datasets_sizes[phase]\n",
        "\n",
        "            losses[phase].append(epoch_loss)\n",
        "            accs[phase].append(epoch_acc)\n",
        "\n",
        "            pbar.set_description('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict()\n",
        "  \n",
        "    time_elapsed = time.time() - start\n",
        "    \n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60\n",
        "    ))\n",
        "    print('Best val Acc: {:.4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    return model, losses, accs"
      ],
      "metadata": {
        "id": "uY8EKyltxpLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(losses, accs):\n",
        "    '''\n",
        "    \n",
        "    plots some metrics graphics\n",
        "    \n",
        "    '''\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses['train'])\n",
        "    plt.plot(losses['val'])\n",
        "    plt.title('loss')\n",
        "    plt.legend(list(losses.keys()))\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(accs['train'])\n",
        "    plt.plot(accs['val'])\n",
        "    plt.title('accuracy')\n",
        "    plt.legend(list(accs.keys()))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "w957ODIIxpNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    '''\n",
        "\n",
        "    count accuracy on a test set\n",
        "\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    curr_correct = 0\n",
        "    for data in dataloader['val']:\n",
        "        inputs, labels = data\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    output = model(inputs)\n",
        "    _, preds = torch.max(output, 1)\n",
        "\n",
        "    curr_correct += int(torch.sum(preds == labels))\n",
        "\n",
        "    return curr_correct / datasets_sizes['val']"
      ],
      "metadata": {
        "id": "96gAXcsRxpPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader_test):\n",
        "    '''\n",
        "    \n",
        "    predicting test set\n",
        "\n",
        "    '''\n",
        "    probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for inputs, y in tqdm(dataloader_test):\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = inputs.cuda()\n",
        "                \n",
        "            preds = model(inputs).cpu()\n",
        "            probs.append(preds)\n",
        "            \n",
        "    print(f'probs shape before softmax: {len(probs)}')\n",
        "    probs = nn.functional.softmax(torch.cat(probs), dim=-1).numpy()\n",
        "    print(f'probs shape after softmax: {probs.shape}')\n",
        "    \n",
        "    return probs"
      ],
      "metadata": {
        "id": "o6zxHyJuxpSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "DUCTbt3ayIdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet = models.resnet152(pretrained=True)\n",
        "\n",
        "for param in model_resnet.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "num_features = model_resnet.fc.in_features\n",
        "model_resnet.classifier = nn.Linear(num_features, len(full_data.classes))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model_resnet = model_resnet.cuda()\n",
        "    print('Training with cuda')\n",
        "    \n",
        "model_resnet.classifier"
      ],
      "metadata": {
        "id": "Sks7EA3wxpUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(list(model_resnet.parameters()), lr=1e-4)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "\n",
        "model_resnet, losses_resnet, accs_resnet = train_model(model_resnet, loss_func, optimizer, exp_lr_scheduler, epochs=12)"
      ],
      "metadata": {
        "id": "WzFbTVUdxpV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(losses_resnet, accs_resnet)"
      ],
      "metadata": {
        "id": "JRxJspRlyP49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG"
      ],
      "metadata": {
        "id": "7DIRc0VByTuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_vgg = models.vgg16(pretrained=True)\n",
        "\n",
        "# layers_to_unfreeze = 7\n",
        "\n",
        "# for param in model_vgg.features[:-layers_to_unfreeze].parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# num_features = 25088\n",
        "# model_vgg.classifier = nn.Linear(num_features, len(full_data.classes))\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     model_vgg = model_vgg.cuda()\n",
        "#     print('Training with cuda')\n",
        "    \n",
        "# model_vgg.classifier"
      ],
      "metadata": {
        "id": "UPrIXaUpyP99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(list(model_vgg.features.parameters())[-layers_to_unfrelayers_to_unfreeze+ \n",
        "#                        list(model_vgg.classifier.parameters()), lr=1e-4)\n",
        "\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# model_vgg, losses_vgg, accs_vgg = train_model(model_vgg, loss_func, optimizer, exp_lr_scheduler, epochs=10)"
      ],
      "metadata": {
        "id": "_HdhYkOAyQAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_metrics(losses_vgg, accs_vgg)"
      ],
      "metadata": {
        "id": "Nc7I8r27yQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, classes = next(iter(dataloader_test))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out)"
      ],
      "metadata": {
        "id": "FoOPXXS0yejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_filenames = [fn[0].split('/')[-1] for fn in dataset_test.imgs]\n",
        "\n",
        "probs_resnet = predict(model_resnet, dataloader_test)\n",
        "print(probs_resnet.shape)\n",
        "\n",
        "preds_resnet = np.argmax(probs_resnet, axis=1)\n",
        "preds_resnet = class_names[preds_resnet]\n",
        "\n",
        "pred_data = pd.read_csv('../journey-springfield/sample_submission.csv')\n",
        "pred_data = pd.DataFrame({'id': test_filenames, 'Expected': preds_resnet}).sort_values('id')\n",
        "pred_data.to_csv('./pred_data.csv', index=False)\n",
        "\n",
        "pred_data"
      ],
      "metadata": {
        "id": "-fl48aKVyjf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}